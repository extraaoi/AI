{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise (embeddings)\n",
    "\n",
    "In this exercise you'll answer some questions about embeddings and experiment with a couple variations on the movie rating prediction model we trained in the [tutorial](#$TUTORIAL_URL$).\n",
    "\n",
    "Run the cell below to do some setup. (It will take a minute or two to run, since we're training a couple models for use later in the exercise. While it's running, you can jump ahead and start on part 1.)\n",
    "\n",
    "> *Aside*: Training the model in the tutorial took around a minute per epoch. To make training time more manageable, we'll be working with a sample of 10% of the total dataset (or around 2 million ratings). We've also limited our sample to relatively popular movies (movies with more than 1k ratings in the full dataset). For these reasons, our results in this exercise won't be directly comparable to the error rates reported in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code. Make sure you run this first!\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from learntools.core import binder; binder.bind(globals())\n",
    "from learntools.embeddings.ex1_embedding_layers import *\n",
    "\n",
    "#_RM_\n",
    "input_dir = '../input/movielens_preprocessed'\n",
    "#_UNCOMMENT_\n",
    "#input_dir = '../input'\n",
    "\n",
    "# Load a 10% subset of the full MovieLens data.\n",
    "df = pd.read_csv(os.path.join(input_dir, 'mini_rating.csv'))\n",
    "\n",
    "# Some hyperparameters. (You might want to play with these later)\n",
    "LR = .005 # Learning rate\n",
    "EPOCHS = 8 # Default number of training epochs (i.e. cycles through the training data)\n",
    "hidden_units = (32,4) # Size of our hidden layers\n",
    "\n",
    "def build_and_train_model(movie_embedding_size=8, user_embedding_size=8, verbose=2, epochs=EPOCHS):\n",
    "    tf.set_random_seed(1); np.random.seed(1); random.seed(1) # Set seeds for reproducibility\n",
    "\n",
    "    user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "    movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "    user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                            input_length=1, name='movie_embedding')(movie_id_input)\n",
    "    concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "    out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "        out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs = [user_id_input, movie_id_input],\n",
    "        outputs = out,\n",
    "    )\n",
    "    model.compile(\n",
    "        tf.train.AdamOptimizer(LR),\n",
    "        loss='MSE',\n",
    "        metrics=['MAE'],\n",
    "    )\n",
    "    history = model.fit(\n",
    "        [df.userId, df.movieId],\n",
    "        df.y,\n",
    "        batch_size=5 * 10**3,\n",
    "        epochs=epochs,\n",
    "        verbose=verbose,\n",
    "        validation_split=.05,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Train two models with different embedding sizes and save the training statistics.\n",
    "# We'll be using this later in the exercise.\n",
    "history_8 = build_and_train_model(verbose=0)\n",
    "history_32 = build_and_train_model(32, 32, verbose=0)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: When to use embeddings?\n",
    "\n",
    "You're a data scientist for the hot new music streaming service, Tidify All Accessâ„¢ with the task of building a machine learning model to generate suggested songs to show users. You have lots of historical data about songs that users have listened to in the past. The dataset of 500 million streams includes the following variables:\n",
    "\n",
    "| Variable name | Description                                       | Number of unique values | Example values\n",
    "|---------------|---------------------------------------------------|------|----------------------------------\n",
    "| stream_id     | Unique id for this streaming event (i.e. per row) | 500m | 480744481269, 228441807745, 182969356277, ...\n",
    "| user_id       | Unique id for this user                           | 5m   | 3592022173, 2596402742, 3506743568, ...\n",
    "| song_id       | Unique id for this song                           | 10m  | 3150630225,  590655137, 3617674674, ...\n",
    "| timestamp     | When did the song start playing?                  | 80m  | 10/08/2017 01:20:44 PM, 04/22/2017 01:58:59 PM, ...\n",
    "| artist_id     | Unique id for the recording artist of this song   | 1m   | 122168143,  52958907, 803608525, ...\n",
    "| song_duration | How long is the song (in seconds)?                | 450  | 257, 155, 212...\n",
    "| explicit      | Is this song flagged as having adult language?    | 2    | True, False\n",
    "| user_country  | Where is this user located? (3-letter ISO country code) | 150  | CAN, KOR, CHE...\n",
    "\n",
    "If you wanted to train a neural net on streaming data, which of the variables above would you include and use use an embedding layer for? Use the variable `embedding_variables` below to give your answer.\n",
    "\n",
    "(You can assume that, if necessary, string variables can be converted to unique numerical identifiers as a preprocessing step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_variables should contain all the variables you would use an embedding layer for\n",
    "# For your convenience, we've initialized it with all variables in the dataset, so you can \n",
    "# just delete or comment out the variables you want to exclude.\n",
    "embedding_variables = {\n",
    "    'stream_id',\n",
    "    'user_id',\n",
    "    'song_id',\n",
    "    'timestamp',\n",
    "    'artist_id',\n",
    "    'song_duration',\n",
    "    'explicit',\n",
    "    'user_country',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Incorrect\n",
    "embedding_variables = {\n",
    "    'stream_id',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Correct 1\n",
    "embedding_variables = {\n",
    "    'user_id',\n",
    "    'song_id',\n",
    "    'artist_id',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Correct 2 (optional extra val)\n",
    "embedding_variables = {\n",
    "    'user_id',\n",
    "    'song_id',\n",
    "    'artist_id',\n",
    "    'user_country',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Choosing embedding sizes\n",
    "\n",
    "In the [tutorial](#$TUTORIAL_URL$), we (somewhat arbitrarily) set `output_dim=8` when creating our movie and user embedding layers. Run the code cell below to see a visualization of our training and validation error over 8 epochs of training, again using 8-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_FS = (15, 5)\n",
    "def plot_history(histories, keys=('mean_absolute_error',), train=True, figsize=history_FS):\n",
    "    if isinstance(histories, tf.keras.callbacks.History):\n",
    "        histories = [ ('', histories) ]\n",
    "    for key in keys:\n",
    "        plt.figure(figsize=history_FS)\n",
    "        for name, history in histories:\n",
    "            val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                           '--', label=str(name).title()+' Val')\n",
    "            if train:\n",
    "                plt.plot(history.epoch, history.history[key], color=val[0].get_color(), alpha=.5,\n",
    "                         label=str(name).title()+' Train')\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(key.replace('_',' ').title())\n",
    "        plt.legend()\n",
    "        plt.title(key)\n",
    "\n",
    "        plt.xlim([0,max(max(history.epoch) for (_, history) in histories)])\n",
    "\n",
    "plot_history([ \n",
    "    ('base model', history_8),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of the notebook we also trained a model with 64-dimensional movie and user embeddings. How do you expect the results to compare? Make a prediction, then run the cell below to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([ \n",
    "    ('8-d embeddings', history_8),\n",
    "    ('32-d embeddings', history_32),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're feeling experimental, feel free to try some other configurations. So far we've varied movie and user embedding size in lock step, but there's no reason they have to be the same. Do you have an intuition about whether one or the other should be bigger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: shrinking movie embeddings and growing user embeddings\n",
    "#history_biguser_smallmovie = build_and_train_model(movie_embedding_size=4, user_embedding_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "history_biguser_smallmovie = build_and_train_model(movie_embedding_size=4, user_embedding_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "plot_history([ \n",
    "    ('8-d embeddings', history_8),\n",
    "    ('32-d embeddings', history_32),\n",
    "    ('m4u16', history_biguser_smallmovie),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're ready, uncomment the cell below for some explanation of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part2.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adding biases\n",
    "\n",
    "In this final section, you'll implement a modification to our model's architecture: per-movie biases.\n",
    "\n",
    "In ML-speak, a **bias** is just a number that gets added to a node's output value. For each movie, we'll learn a single number that we'll add to the output of what was previously our final node. Here's what that looks like:\n",
    "\n",
    "<img src=\"https://docs.google.com/a/google.com/drawings/d/e/2PACX-1vSmf5H7Rcr771flhidl7Wf31OXiZTUgNH2qzoVc-2dtH6Cf9XmSF3xQcY7m1RwCRBu2_lE-dH5Vb6ny/pub?w=1050&h=594\" />\n",
    "\n",
    "### Why?\n",
    "\n",
    "Do you think this will improve the accuracy of our predictions? Why or why not?\n",
    "\n",
    "Think about it, then uncomment the line below to see an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.a.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have an idea about what the bias values will look like? Are there certain movies you expect will have high or low biases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.b.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding up biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code we used to train our embedding model in the tutorial. Modify it (where indicated by the comments) to add per-movie biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding_size = movie_embedding_size = 8\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating (before adding bias)\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "################################################################################\n",
    "############################# YOUR CODE GOES HERE! #############################\n",
    "# TODO: you need to create the variable movie_bias. Its value should be the output of calling a layer.\n",
    "# I recommend giving the layer that holds your biases a distinctive name (this will help in an upcoming question)\n",
    "#movie_bias = \n",
    "################################################################################\n",
    "out = keras.layers.Add()([out, movie_bias])\n",
    "\n",
    "model_bias = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model_bias.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")\n",
    "model_bias.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No idea where to start? Don't panic! Uncomment and run the line below for some hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.c.hint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.c.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Bad solution (no bias adding)\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "bias_embedded = keras.layers.Embedding(df.movieId.max()+1, 1, input_length=1, name='bias',\n",
    "                                      )(movie_id_input)\n",
    "movie_bias = keras.layers.Flatten()(bias_embedded)\n",
    "#out = keras.layers.Add()([out, movie_bias])\n",
    "\n",
    "model_bias = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model_bias.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")\n",
    "part3.c.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Canonical correct solution\n",
    "user_embedding_size = movie_embedding_size = 8\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating (before adding bias)\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "################################################################################\n",
    "############################# YOUR CODE GOES HERE! #############################\n",
    "# TODO: you need to create the variable movie_bias. Its value should be the output of calling a layer.\n",
    "# I recommend giving the layer that holds your biases a distinctive name (this will help in an upcoming question)\n",
    "bias_embedded = keras.layers.Embedding(df.movieId.max()+1, 1, input_length=1, name='bias',\n",
    "                                      )(movie_id_input)\n",
    "movie_bias = keras.layers.Flatten()(bias_embedded)\n",
    "################################################################################\n",
    "out = keras.layers.Add()([out, movie_bias])\n",
    "\n",
    "model_bias = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model_bias.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")\n",
    "part3.c.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Run the code below to train the model you built in the previous step. (If you get an error when trying to fit your model, you may have made a mistake in your bias implementation. See `part3.c.solution()` for a working implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bias = model_bias.fit(\n",
    "    [df.userId, df.movieId],\n",
    "    df.y,\n",
    "    batch_size=5 * 10**3,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=2,\n",
    "    validation_split=.05,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it compare to the results we got from the model without biases? Run the code cell below to compare their loss over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([ \n",
    "    ('no_bias', history_8),\n",
    "    ('bias', history_bias),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did adding biases affect our results?\n",
    "\n",
    "Averaged over many runs, biases seem to help a bit, but there's enough variance between runs (as a result of different random initializations), that you might be seeing better or worse results. If you're seeing a *big* difference (more than, say, +-.02 in the final loss) in either direction, something may have gone wrong.\n",
    "\n",
    "So biases weren't the huge win we might have hoped for, but it still seems worth testing our hypothesis about how bias values will be distributed among movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting learned bias values\n",
    "\n",
    "Let's take a look at the bias values our model has learned. Fill in the missing code in the cell below to load an array of bias values - `b` should be an array with one number for each movie in our training set.\n",
    "\n",
    "**Hint:** you may want to check out the docs for [`keras.Model.get_layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#get_layer). This will be easier if you gave your `Embedding` bias layer a distinctive name in part 2. If you didn't, it may help to look at the output of `model_bias.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_layer = None\n",
    "\n",
    "part3.d.check()\n",
    "\n",
    "(b,) = bias_layer.get_weights()\n",
    "print(\"Loaded biases with shape {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.d.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Bad solution (wrong layer)\n",
    "bias_layer = model_bias.get_layer(index=10)\n",
    "part3.d.check()\n",
    "\n",
    "(b,) = bias_layer.get_weights()\n",
    "print(\"Loaded biases with shape {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Canonical correct solution\n",
    "bias_layer = model_bias.get_layer('bias')\n",
    "part3.d.check()\n",
    "\n",
    "(b,) = bias_layer.get_weights()\n",
    "print(\"Loaded biases with shape {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've successfully set the value of `bias_layer`, run the cell below which loads a dataframe containing movie metadata and adds the biases found in the previous step as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(os.path.join(input_dir, 'movie.csv'), index_col=0, \n",
    "                     usecols=['movieId', 'title', 'genres', 'year'])\n",
    "ntrain = math.floor(len(df) * .95)\n",
    "df_train = df.head(ntrain)\n",
    "\n",
    "# Mapping from original movie ids to canonical ones\n",
    "mids = movieId_to_canon = df.groupby('movieId')['movieId_orig'].first()\n",
    "# Add bias column\n",
    "movies.loc[mids.values, 'bias'] = b\n",
    "# Add columns for number of ratings and average rating\n",
    "g = df_train.groupby('movieId_orig')\n",
    "movies.loc[mids.values, 'n_ratings'] = g.size()\n",
    "movies.loc[mids.values, 'mean_rating'] = g['rating'].mean()\n",
    "\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which movies have the lowest and highest learned biases? Run the cell below to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "n = 10\n",
    "display(\n",
    "    \"Largest biases...\",\n",
    "    movies.sort_values(by='bias', ascending=False).head(n),\n",
    "    \"Smallest biases...\",\n",
    "    movies.sort_values(by='bias').head(n),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate a scatter plot of movies' average ratings against the biases learned for those movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "mini = movies.sample(n, random_state=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "ax.scatter(mini['mean_rating'], mini['bias'], alpha=.4)\n",
    "ax.set_xlabel('Mean rating')\n",
    "ax.set_ylabel('Bias');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Experiment: Draw line of expected fit\n",
    "n = 1000\n",
    "mini = movies.sample(n, random_state=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "ax.scatter(mini['mean_rating'], mini['bias'], alpha=.4)\n",
    "\n",
    "row = df.iloc[0]\n",
    "global_mean_rating = row.rating - row.y\n",
    "lw = 1\n",
    "ax.axhline(0, ls='--', color='grey', lw=lw)\n",
    "ax.axvline(global_mean_rating, ls='--', label='Overall mean rating', color='indigo', lw=lw)\n",
    "# Draw line of expected fit (if biases were to match data means)\n",
    "pt = (global_mean_rating, 0)\n",
    "x0, y0 = 0.5, (0.5 - global_mean_rating)\n",
    "x1, y1 = 5, (5 - global_mean_rating)\n",
    "ylim0 = ax.get_ylim()\n",
    "ax.plot([x0, x1], [y0, y1], linestyle='-', lw=3, color='lime', alpha=.5)\n",
    "ax.set_ylim(ylim0)\n",
    "\n",
    "ax.set_xlabel('Mean rating')\n",
    "ax.set_ylabel('Bias');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering this plot and the list of our highest and lowest bias movies, do our model's learned biases agree with what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM%%\n",
    "# (Discussion in solution text doesn't really make sense anymore now that we're thresholding obscure movies\n",
    "# for this exercise.)\n",
    "#_COMMENT_IF(PROD)_\n",
    "part3.e.solution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "learntools_metadata": {
   "lesson_index": 0,
   "type": "exercise"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
