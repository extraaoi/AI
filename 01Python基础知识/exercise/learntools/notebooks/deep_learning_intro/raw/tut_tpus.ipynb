{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for the Higgs Boson #\n",
    "\n",
    "The Standard Model is a theory in particle physics that describes some of the most basic forces of nature. One fundamental particle, the Higgs boson, is what accounts for the *mass* of matter. First theorized in the 1964, the Higgs boson eluded observation for almost fifty years. In 2012 it was finally observed experimentally at the Large Hadron Collider. These experiments produced millions of gigabytes of data.\n",
    "\n",
    "Large and complicated datasets like these are where deep learning excels. In this notebook, we'll build a Wide and Deep neural network to determine whether an observed particle collision produced a Higgs boson or not.\n",
    "\n",
    "# The Collision Data #\n",
    "\n",
    "The collision of protons at high energy can produce new particles like the Higgs boson. These particles can't be directly observed, however, since they decay almost instantly. So to detect the presence of a new particle, we instead obesrve the behavior of the particles they decay into, their \"decay products\".\n",
    "\n",
    "The *Higgs* dataset contains 21 \"low-level\" features of the decay products and also 7 more \"high-level\" features derived from these.\n",
    "\n",
    "# Wide and Deep Neural Networks #\n",
    "\n",
    "A *Wide and Deep* network trains a linear layer side-by-side with a deep stack of dense layers. Wide and Deep networks are often effective on tabular datasets.[^1]\n",
    "\n",
    "Both the dataset and the model are much larger than what we used in the course. To speed up training, we'll use Kaggle's [Tensor Processing Units](https://www.kaggle.com/docs/tpu) (TPUs), an accelerator ideal for large workloads.\n",
    "\n",
    "We've collected some hyperparameters here to make experimentation easier. Fork this notebook by [**clicking here**](https://www.kaggle.com/kernels/fork/12171965) to try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "UNITS = 2 ** 11 # 2048\n",
    "ACTIVATION = 'relu'\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training Configuration\n",
    "BATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few sections set up the TPU computation, data pipeline, and neural network model. If you'd just like to see the results, feel free to skip to the end!\n",
    "\n",
    "# Setup #\n",
    "\n",
    "In addition to our imports, this section contains some code that will connect our notebook to the TPU and create a **distribution strategy**. Each TPU has eight computational cores acting independently. With a distribution strategy, we define how we want to divide up the work between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "# TF 2.3 version\n",
    "# Detect and init the TPU\n",
    "# try: # detect TPUs\n",
    "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n",
    "#     strategy = tf.distribute.TPUStrategy(tpu)\n",
    "# except ValueError: # detect GPUs\n",
    "#     strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "# print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# TF 2.2 version\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
    "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
    "    \n",
    "# Plotting\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib defaults\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "\n",
    "\n",
    "# Data\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from tensorflow.io import FixedLenFeature\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "# Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that TensorFlow now detects eight accelerators. Using a TPU is a bit like using eight GPUs at once.\n",
    "\n",
    "# Load Data #\n",
    "\n",
    "The dataset has been encoded in a binary file format called *TFRecords*. These two functions will parse the TFRecords and build a TensorFlow `tf.data.Dataset` object that we can use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(feature_description):\n",
    "    def decoder(example):\n",
    "        example = tf.io.parse_single_example(example, feature_description)\n",
    "        features = tf.io.parse_tensor(example['features'], tf.float32)\n",
    "        features = tf.reshape(features, [28])\n",
    "        label = example['label']\n",
    "        return features, label\n",
    "    return decoder\n",
    "\n",
    "def load_dataset(filenames, decoder, ordered=False):\n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False\n",
    "    dataset = (\n",
    "        tf.data\n",
    "        .TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
    "        .with_options(ignore_order)\n",
    "        .map(decoder, AUTO)\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = int(11e6)\n",
    "validation_size = int(5e5)\n",
    "training_size = dataset_size - validation_size\n",
    "\n",
    "# For model.fit\n",
    "batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "steps_per_epoch = training_size // batch_size\n",
    "validation_steps = validation_size // batch_size\n",
    "\n",
    "# For model.compile\n",
    "steps_per_execution = steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'features': FixedLenFeature([], tf.string),\n",
    "    'label': FixedLenFeature([], tf.float32),\n",
    "}\n",
    "decoder = make_decoder(feature_description)\n",
    "\n",
    "data_dir = KaggleDatasets().get_gcs_path('higgs-boson')\n",
    "train_files = tf.io.gfile.glob(data_dir + '/training' + '/*.tfrecord')\n",
    "valid_files = tf.io.gfile.glob(data_dir + '/validation' + '/*.tfrecord')\n",
    "\n",
    "ds_train = load_dataset(train_files, decoder, ordered=False)\n",
    "ds_train = (\n",
    "    ds_train\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .shuffle(2 ** 19)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "ds_valid = load_dataset(valid_files, decoder, ordered=False)\n",
    "ds_valid = (\n",
    "    ds_valid\n",
    "    .batch(batch_size)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #\n",
    "\n",
    "Now that the data is ready, let's define the network. We're defining the deep branch of the network using Keras's *Functional API*, which is a bit more flexible that the `Sequential` method we used in the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_block(units, activation, dropout_rate, l1=None, l2=None):\n",
    "    def make(inputs):\n",
    "        x = layers.Dense(units)(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(activation)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        return x\n",
    "    return make\n",
    "\n",
    "with strategy.scope():\n",
    "    # Wide Network\n",
    "    wide = keras.experimental.LinearModel()\n",
    "\n",
    "    # Deep Network\n",
    "    inputs = keras.Input(shape=[28])\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    deep = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Wide and Deep Network\n",
    "    wide_and_deep = keras.experimental.WideDeepModel(\n",
    "        linear_model=wide,\n",
    "        dnn_model=deep,\n",
    "        activation='sigmoid',\n",
    "    )\n",
    "\n",
    "wide_and_deep.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['AUC', 'binary_accuracy'],\n",
    "#     experimental_steps_per_execution=steps_per_execution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training #\n",
    "\n",
    "During training, we'll use the `EarlyStopping` callback as usual. Notice that we've also defined a **learning rate schedule**. It's been found that gradually decreasing the learning rate over the course of training can improve performance (the weights \"settle in\" to a minimum). This schedule will multiply the learning rate by `0.2` if the validation loss didn't decrease after an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "early_stopping = callbacks.EarlyStopping(\n",
    "    patience=2,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_schedule = callbacks.ReduceLROnPlateau(\n",
    "    patience=0,\n",
    "    factor=0.2,\n",
    "    min_lr=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = wide_and_deep.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_valid,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, lr_schedule],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_frame = pd.DataFrame(history.history)\n",
    "history_frame.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\n",
    "history_frame.loc[:, ['auc', 'val_auc']].plot(title='AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References #\n",
    "\n",
    "- Baldi, P. et al. *Searching for Exotic Particles in High-Energy Physics with Deep Learning*. (2014) ([arXiv](https://arxiv.org/abs/1402.4735))\n",
    "- Cheng, H. et al. *Wide & Deep Learning for Recommender Systems*. (2016) ([arXiv](https://arxiv.org/abs/1606.07792))\n",
    "- *What Exactly is the Higgs Boson?* Scientific American. (1999) [(article)](https://www.scientificamerican.com/article/what-exactly-is-the-higgs/)]\n",
    "\n",
    "[^1]: In the original implementation, categorical features were one-hot encoded and crossed to produce the interaction features. This \"wide\" dataset was used with the linear component. For the deep component, the categories were encoded into a much narrower embedding layer."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
