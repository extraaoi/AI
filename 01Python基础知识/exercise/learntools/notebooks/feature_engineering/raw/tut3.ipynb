{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Creating new features from the raw data is one of the best ways to improve your model. For example, when working with Kickstarter data, you can calculate the number of total projects in the last week and the duration of the fundraising period. The features you create are different for every dataset, so it takes a bit of creativity and experimentation. We're a bit limited here, since we're working with only one table. Typically you'll have access to multiple tables with relevant data that you can use to create new features.\n",
    "\n",
    "But you can still see how to make new features using categorical features, and then a few examples of generated numerical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$HIDE_INPUT$\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ks = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv',\n",
    "                 parse_dates=['deadline', 'launched'])\n",
    "\n",
    "# Drop live projects\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n",
    "\n",
    "# Timestamp features\n",
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)\n",
    "\n",
    "# Label encoding\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "encoded = ks[cat_features].apply(encoder.fit_transform)\n",
    "\n",
    "data_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\n",
    "baseline_data = ks[data_cols].join(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactions\n",
    "\n",
    "One of the easiest ways to create new features is by combining categorical variables. For example, if one record has the country `\"CA\"` and category `\"Music\"`, you can create a new value `\"CA_Music\"`. This is a new categorical feature that can provide information about correlations between categorical variables. This type of feature is typically called an **interaction**. \n",
    "\n",
    "In general, you would build interaction features from all pairs of categorical features. You can make interactions from three or more features as well, but you'll tend to get diminishing returns.\n",
    "\n",
    "Pandas lets us simply add string columns together like normal Python strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = ks['category'] + \"_\" + ks['country']\n",
    "print(interactions.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can label encode the interaction feature and add it to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()\n",
    "data_interaction = baseline_data.assign(category_country=label_enc.fit_transform(interactions))\n",
    "data_interaction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise, you'll build interaction terms for all pairs of categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of projects in the last week\n",
    "\n",
    "Next, we'll count the number of projects launched in the preceeding week for each record. I'll use the `.rolling` method on a series with the `\"launched\"` column as the index. I'll create the series, using `ks.launched` as the index and `ks.index` as the values, then sort the times. Using a time series as the index allows us to define the rolling window size in terms of hours, days, weeks, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a Series with a timestamp index\n",
    "launched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\n",
    "launched.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are seven projects that have obviously wrong launch dates, but we'll just ignore them. Again, this is something you'd handle when cleaning the data, but it's not the focus of this mini-course.\n",
    "\n",
    "With a timeseries index, you can use `.rolling` to select time periods as the window. For example `launched.rolling('7d')` creates a rolling window that contains all the data in the previous 7 days. The window contains the current record, so if we want to count all the *previous* projects but not the current one, we'll need to subtract 1. We'll plot the results to make sure it looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days = launched.rolling('7d').count() - 1\n",
    "print(count_7_days.head(20))\n",
    "\n",
    "# Ignore records with broken launch dates\n",
    "plt.plot(count_7_days[7:]);\n",
    "plt.title(\"Number of projects launched over periods of 7 days\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the counts, we need to adjust the index so we can join it with the other training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_7_days.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now join the new feature with the other data again using `.join` since we've matched the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data.join(count_7_days).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time since the last project in the same category\n",
    "\n",
    "Do projects in the same category compete for donors? If you're trying to fund a video game and another game project was just launched, you might not get as much money. We can capture this by calculating the time since the last launch project in the same category.\n",
    "\n",
    "A handy method for performing operations within groups is to use `.groupby` then `.transform`. The `.transform` method takes a function then passes a series or dataframe to that function for each group. This returns a dataframe with the same indices as the original dataframe. In our case, we'll perform a groupby on `\"category\"` and use transform to calculate the time differences for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since_last_project(series):\n",
    "    # Return the time in hours\n",
    "    return series.diff().dt.total_seconds() / 3600.\n",
    "\n",
    "df = ks[['category', 'launched']].sort_values('launched')\n",
    "timedeltas = df.groupby('category').transform(time_since_last_project)\n",
    "timedeltas.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get `NaN`s here for projects that are the first in their category. We'll need to fill those in with something like the mean or median. We'll also need to reset the index so we can join it with the other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final time since last project\n",
    "timedeltas = timedeltas.fillna(timedeltas.median()).reindex(baseline_data.index)\n",
    "timedeltas.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the values in `\"goal\"` shows that most projects have goals less than 5000 USD. However, there is a long tail of goals going up to $100,000. Some models work better when the features are normally distributed, so it might help to transform the goal values. Common choices for this are the square root and natural logarithm. These transformations can also help constrain outliers.\n",
    "\n",
    "Here I'll transform the goal feature using the square root and log functions as examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ks.goal, range=(0, 100000), bins=50);\n",
    "plt.title('Goal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sqrt(ks.goal), range=(0, 400), bins=50);\n",
    "plt.title('Sqrt(Goal)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(ks.goal), range=(0, 25), bins=50);\n",
    "plt.title('Log(Goal)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log transformation won't help our model since tree-based models are scale invariant. However, this should help if we had a linear model or neural network.\n",
    "\n",
    "Other transformations include squares and other powers, exponentials, etc. These might help the model discriminate, like the kernel trick for SVMs. Again, it takes a bit of experimentation to see what works. One method is to create a bunch of new features and later choose the best ones with feature selection algorithms.\n",
    "\n",
    "# Your Turn\n",
    "**[Try your hand at generating features](#$NEXT_NOTEBOOK_URL$)** to improve performance on your model from the previous exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
